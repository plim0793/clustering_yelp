{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Yelp Data\n",
    "\n",
    "Paul Lim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/plim0793/anaconda/lib/python3.5/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "# Main imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "# sklearn\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import pipeline, feature_selection, decomposition\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering, Birch\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# NLP \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim import models\n",
    "from gensim.models import word2vec\n",
    "import snowballstemmer\n",
    "\n",
    "# Misc.\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "import logging\n",
    "import math\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set_style('ticks')\n",
    "sns.set_style({'xtick.direction': u'in', 'ytick.direction': u'in'})\n",
    "sns.set_style({'legend.frameon': True})\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running List of Functions/Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataframeToSeriesTransformer(BaseEstimator, TransformerMixin):\n",
    "        \n",
    "    def __init__(self, col=None):\n",
    "        self.col = col\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.col:\n",
    "            print(\"DTST: \", X[self.col].shape)\n",
    "            return X[self.col]\n",
    "        else:\n",
    "            return X\n",
    "        \n",
    "class SeparateFeaturesTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, num_cols=None, text_cols=None):\n",
    "        self.num_cols = num_cols\n",
    "        self.text_cols = text_cols\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.num_cols:\n",
    "            print(\"SFT: \", X.loc[:, self.num_cols].shape)\n",
    "            return X.loc[:, self.num_cols]\n",
    "        elif self.text_cols:\n",
    "            print(\"SFT: \", X.loc[:, self.text_cols].shape)\n",
    "            return X.loc[:, self.text_cols]\n",
    "        else:\n",
    "            return X\n",
    "        \n",
    "class WilsonAverageTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, num_col=None, biz_list=None):\n",
    "        self.num_col = num_col\n",
    "        self.biz_list = biz_list\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.num_col and self.biz_list.all():\n",
    "            scores = get_average_rating(X, self.biz_list)\n",
    "            \n",
    "            X_avg = pd.DataFrame({'average': scores})\n",
    "            print(\"WAT: \", X_avg.shape)\n",
    "            return X_avg\n",
    "        else:\n",
    "            return X\n",
    "        \n",
    "class CleanTextTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, text_col=None):\n",
    "        self.text_col = text_col\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "#         NLP = spacy.load('en')\n",
    "        stemmer = snowballstemmer.EnglishStemmer()\n",
    "        stop = stopwords.words('english')\n",
    "        stop_list = stemmer.stemWords(stop)\n",
    "        stop_list = set(stop_list)\n",
    "        stop = set(stop + list(stop_list))\n",
    "        \n",
    "        if self.text_col:\n",
    "            df = pd.DataFrame()\n",
    "            clean_review_list = []\n",
    "            \n",
    "            for review in X.loc[:, self.text_col]:\n",
    "                clean_review = ''\n",
    "                \n",
    "                for word in TextBlob(review).words:\n",
    "                    if word not in stop:\n",
    "                        clean_review += word.lemmatize() + ' '\n",
    "                        \n",
    "#                 clean_review = NLP(clean_review)\n",
    "                clean_review_list.append(clean_review)\n",
    "                        \n",
    "            df['clean_reviews'] = clean_review_list\n",
    "            print(\"CTT: \", df.shape)\n",
    "            return df\n",
    "        else:\n",
    "            return X\n",
    "        \n",
    "class DensifyTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = pd.DataFrame(X.toarray())\n",
    "        print(\"DT: \", df.shape)\n",
    "        return df\n",
    "    \n",
    "class SentimentTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, text_col=None):\n",
    "        self.text_col = text_col\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.text_col:\n",
    "            df = pd.DataFrame()\n",
    "            sum_pol_list = []\n",
    "            sum_sub_list = []\n",
    "\n",
    "            for doc in X.loc[:, self.text_col]:\n",
    "                sum_pol = 0\n",
    "                sum_sub = 0\n",
    "                doc_blob = TextBlob(doc)\n",
    "\n",
    "                for sent in doc_blob.sentences:\n",
    "                    sum_pol += sent.sentiment[0]\n",
    "                    sum_sub += sent.sentiment[1]\n",
    "\n",
    "                sum_pol_list.append(sum_pol)\n",
    "                sum_sub_list.append(sum_sub)\n",
    "\n",
    "            df['pol'] = sum_pol_list\n",
    "            df['sub'] = sum_sub_list\n",
    "            df['clean_reviews'] = X.loc[:, self.text_col] # Need to keep the clean reviews for the W2V transformer.\n",
    "            print(\"ST: \", df.shape)\n",
    "            return df\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, text_col=None, w2v=None):\n",
    "        self.text_col = text_col\n",
    "        self.w2v = w2v\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.text_col:\n",
    "            avg_w2v_list = []\n",
    "            \n",
    "            for review in X.loc[:, self.text_col]:\n",
    "                avg_w2v = np.zeros(300)\n",
    "                count = 0\n",
    "                \n",
    "                for word in review:\n",
    "                    try:\n",
    "                        avg_w2v += w2v.word_vec(word)\n",
    "                        count += 1\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "                avg_w2v_list.append(avg_w2v/count)\n",
    "            df = pd.DataFrame(avg_w2v_list)\n",
    "#             print(df.head())\n",
    "            print(\"W2V: \", df.shape)\n",
    "            return df\n",
    "        else:\n",
    "            return X\n",
    "        \n",
    "class ToDataFrameTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        df = pd.DataFrame(X)\n",
    "#         print(df.head())\n",
    "        print(\"TDFT: \", df.shape)\n",
    "        return df\n",
    "        \n",
    "class DropTextTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, text_col=None):\n",
    "        self.text_col = text_col\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.text_col:\n",
    "            df = X.drop(self.text_col, axis=1)\n",
    "            print(\"DTT: \", df.shape)\n",
    "            return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def confidence(pos, neg):\n",
    "    '''\n",
    "    Calculates the Wilson confidence where pos is the number of positive ratings\n",
    "    and neg is the number of negative ratings.\n",
    "    '''\n",
    "    n = pos + neg\n",
    "    \n",
    "    if n == 0:\n",
    "        return 0\n",
    "    z = 1.96 # 95% confidence interval\n",
    "    phat = float(pos) / n\n",
    "    return (((phat + z*z/(2*n) - z * np.sqrt((phat*(1-phat)+z*z/(4*n))/n))/(1+z*z/n)))\n",
    "\n",
    "def get_average_rating(df, biz_list):\n",
    "    '''\n",
    "    Compiles the list of average ratings for each business in biz_list.\n",
    "    '''\n",
    "    wils_list = []\n",
    "    \n",
    "    for biz in biz_list:\n",
    "        ind_biz = df[df['name'] == biz]\n",
    "        \n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        \n",
    "        for rating in ind_biz['rating']:\n",
    "            if rating > 3:\n",
    "                pos_count += 1\n",
    "            else:\n",
    "                neg_count += 1\n",
    "        \n",
    "        wils_conf = confidence(pos_count, neg_count)\n",
    "        wils_list.append(wils_conf)\n",
    "    return wils_list\n",
    "\n",
    "def sample_cluster(df, group_by_cols, size=5):\n",
    "    sample_fn = lambda obj: obj.loc[np.random.choice(obj.index, size, replace=True), :]\n",
    "    df = df.groupby(group_by_cols, as_index=False).apply(sample_fn)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_metrics(pipe, model, df_orig):\n",
    "    df_transformed = pipe.fit_transform(df_orig)\n",
    "#     print(df_transformed[0])\n",
    "    pred = model.fit_predict(df_transformed)\n",
    "    print(\"Number of Clusters: \", len(np.unique(pred)))\n",
    "    print(\"Silhouette Coefficient: %0.3f\" % silhouette_score(df_transformed, model.labels_))\n",
    "    return df_transformed, pred\n",
    "\n",
    "def model_metrics(model_dict, pipe, df):\n",
    "\n",
    "    model_dfs = {}\n",
    "    for name, model in model_dict.items():\n",
    "        print(name)\n",
    "        temp_df, temp_score = get_metrics(pipe, model, df)\n",
    "        model_dfs[name] = [temp_df, temp_score]\n",
    "    return model_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_init = joblib.load('../data/df_tot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reorder columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_init = df_init[['name', 'rating' ,'reviews']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change rating to int type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_init['rating'] = df_init['rating'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create transformers for splitting text and num columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT:  (204696, 1)\n",
      "                                             reviews\n",
      "0  Hipster coffee galore! What happened to just a...\n",
      "1  A pretty hipster, modern spacious place to do ...\n",
      "2  Great place to grab a coffee and you can get t...\n",
      "3  Coffee is amazing! Overpriced as usual. There'...\n",
      "4  Big time coffee lover. First time customer and...\n",
      "SFT:  (204696, 1)\n",
      "   rating\n",
      "0       2\n",
      "1       4\n",
      "2       5\n",
      "3       4\n",
      "4       4\n"
     ]
    }
   ],
   "source": [
    "t = SeparateFeaturesTransformer(text_cols=['reviews'])\n",
    "n = SeparateFeaturesTransformer(num_cols=['rating'])\n",
    "\n",
    "print(t.transform(df_init).head())\n",
    "print(n.transform(df_init).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT:  (204696, 1)\n",
      "SFT:  (204696, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(204696, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.transform(df_init).shape\n",
    "n.transform(df_init).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create transformer for averaging ratings for each business (may not need this since I might not average across the each business)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Wilson average for one business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_biz = df_init[df_init['name'] == '0_FourBarrelCoffee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_count = 0\n",
    "neg_count = 0\n",
    "\n",
    "for rating in one_biz['rating']:\n",
    "    if rating > 3:\n",
    "        pos_count += 1\n",
    "    else:\n",
    "        neg_count += 1\n",
    "        \n",
    "wil_conf = confidence(pos_count, neg_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalize to all businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "biz_list = df_init['name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wils_scores = get_average_rating(df_init, biz_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the transformer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wat = WilsonAverageTransformer(num_col='rating', biz_list=biz_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAT:  (831, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.726315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.873692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.831345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.733775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.831141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.784363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.720300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.887100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.577846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.938036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.792450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.887100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.612150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.705641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.750164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.646843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.705641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.670207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.733996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.828231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.768344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.748634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.759421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.912809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.779482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.629445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.676079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.747656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.515881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.623669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>0.784363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>0.796769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>0.765006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>0.785916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>0.687863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>0.582880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>0.828231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>0.449550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>0.889004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>0.697998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>0.411443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>0.365458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>0.407660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>0.559595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>0.466971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>0.834958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>0.524230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>0.894198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>0.924440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>0.732655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>0.466971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>0.959063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>0.673268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>0.722083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>0.683030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>0.831890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>0.463366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>0.515881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>0.589239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>0.966670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>831 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      average\n",
       "0    0.726315\n",
       "1    0.873692\n",
       "2    0.831345\n",
       "3    0.733775\n",
       "4    0.831141\n",
       "5    0.784363\n",
       "6    0.720300\n",
       "7    0.887100\n",
       "8    0.577846\n",
       "9    0.938036\n",
       "10   0.792450\n",
       "11   0.887100\n",
       "12   0.612150\n",
       "13   0.705641\n",
       "14   0.750164\n",
       "15   0.646843\n",
       "16   0.705641\n",
       "17   0.670207\n",
       "18   0.733996\n",
       "19   0.828231\n",
       "20   0.768344\n",
       "21   0.748634\n",
       "22   0.759421\n",
       "23   0.912809\n",
       "24   0.779482\n",
       "25   0.629445\n",
       "26   0.676079\n",
       "27   0.747656\n",
       "28   0.515881\n",
       "29   0.623669\n",
       "..        ...\n",
       "801  0.784363\n",
       "802  0.796769\n",
       "803  0.765006\n",
       "804  0.785916\n",
       "805  0.687863\n",
       "806  0.582880\n",
       "807  0.828231\n",
       "808  0.449550\n",
       "809  0.889004\n",
       "810  0.697998\n",
       "811  0.411443\n",
       "812  0.365458\n",
       "813  0.407660\n",
       "814  0.559595\n",
       "815  0.466971\n",
       "816  0.834958\n",
       "817  0.524230\n",
       "818  0.894198\n",
       "819  0.924440\n",
       "820  0.732655\n",
       "821  0.466971\n",
       "822  0.959063\n",
       "823  0.673268\n",
       "824  0.722083\n",
       "825  0.683030\n",
       "826  0.831890\n",
       "827  0.463366\n",
       "828  0.515881\n",
       "829  0.589239\n",
       "830  0.966670\n",
       "\n",
       "[831 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wat.transform(df_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a transformer for preprocessing the reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the lemmatized review for just one review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_review = df_init.iloc[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_doc = nlp(one_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_doc = ''\n",
    "for word in sample_doc:\n",
    "    if word.is_stop == False:\n",
    "        clean_doc += word.lemma_ + ' '\n",
    "clean_doc = nlp(clean_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for sent in clean_doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalize to all reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = df_init.iloc[:100, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ct = CleanText('reviews')\n",
    "\n",
    "test_ct = ct.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer()\n",
    "\n",
    "tf_t = tf.fit_transform(test_ct.clean_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_t.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(test_ct.clean_reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set a smaller test dataframe (10 samples from each business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = sample_cluster(df_init, ['name'], size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4155, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>269</th>\n",
       "      <td>0_FourBarrelCoffee</td>\n",
       "      <td>5</td>\n",
       "      <td>Best coffee, hands down. Parking is a challeng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0_FourBarrelCoffee</td>\n",
       "      <td>5</td>\n",
       "      <td>One of the best almond milk lattes Ive ever ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0_FourBarrelCoffee</td>\n",
       "      <td>4</td>\n",
       "      <td>Great hole-in-the-wall coffee spot. Good coffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0_FourBarrelCoffee</td>\n",
       "      <td>4</td>\n",
       "      <td>Gibraltar ReviewThe Pour:Very good, beans are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0_FourBarrelCoffee</td>\n",
       "      <td>4</td>\n",
       "      <td>TOP notch coffee and the most AMAZING fresh pa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name  rating  \\\n",
       "0 269  0_FourBarrelCoffee       5   \n",
       "  119  0_FourBarrelCoffee       5   \n",
       "  277  0_FourBarrelCoffee       4   \n",
       "  203  0_FourBarrelCoffee       4   \n",
       "  121  0_FourBarrelCoffee       4   \n",
       "\n",
       "                                                 reviews  \n",
       "0 269  Best coffee, hands down. Parking is a challeng...  \n",
       "  119  One of the best almond milk lattes Ive ever ha...  \n",
       "  277  Great hole-in-the-wall coffee spot. Good coffe...  \n",
       "  203  Gibraltar ReviewThe Pour:Very good, beans are ...  \n",
       "  121  TOP notch coffee and the most AMAZING fresh pa...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe_pca = Pipeline([\n",
    "                            ('combined_features', FeatureUnion([\n",
    "\n",
    "                                ('num_feat', SeparateFeaturesTransformer(num_cols=['rating'])),\n",
    "                                ('text_feat', Pipeline([\n",
    "\n",
    "                                    ('split_text', SeparateFeaturesTransformer(text_cols=['reviews'])),\n",
    "                                    ('clean', CleanTextTransformer('reviews')),\n",
    "                                    ('combine_text', FeatureUnion([\n",
    "                                            \n",
    "                                        ('sentiment_analysis', SentimentTransformer(text_col='clean_reviews')),\n",
    "                                        ('vectorize', Pipeline([\n",
    "\n",
    "                                            ('to_series', DataframeToSeriesTransformer(col='clean_reviews')),\n",
    "                                            ('tfidf', TfidfVectorizer(stop_words=\"english\",\n",
    "                                                                      token_pattern=\"\\\\b[a-zA-Z][a-zA-Z]+\\\\b\",\n",
    "                                                                      min_df=10)),\n",
    "                                            ('densify', DensifyTransformer()),\n",
    "                                            ('pca', PCA(n_components=3))\n",
    "                                                            ]))\n",
    "                                                                ])),\n",
    "                                    ('to_df', ToDataFrameTransformer()),\n",
    "                                    ('drop_text', DropTextTransformer(text_col=2))\n",
    "                                                        ]))\n",
    "                                                            ]))\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_nmf = Pipeline([\n",
    "                            ('combined_features', FeatureUnion([\n",
    "\n",
    "                                ('num_feat', SeparateFeaturesTransformer(num_cols=['rating'])),\n",
    "                                ('text_feat', Pipeline([\n",
    "\n",
    "                                    ('split_text', SeparateFeaturesTransformer(text_cols=['reviews'])),\n",
    "                                    ('clean', CleanTextTransformer('reviews')),\n",
    "                                    ('combine_text', FeatureUnion([\n",
    "                                            \n",
    "                                        ('sentiment_analysis', SentimentTransformer(text_col='clean_reviews')),\n",
    "                                        ('vectorize', Pipeline([\n",
    "\n",
    "                                            ('to_series', DataframeToSeriesTransformer(col='clean_reviews')),\n",
    "                                            ('tfidf', TfidfVectorizer(stop_words=\"english\",\n",
    "                                                                      token_pattern=\"\\\\b[a-zA-Z][a-zA-Z]+\\\\b\",\n",
    "                                                                      min_df=10)),\n",
    "                                            ('densify', DensifyTransformer()),\n",
    "                                            ('nmf', NMF(n_components=3))\n",
    "                                                            ]))\n",
    "                                                                ])),\n",
    "                                    ('to_df', ToDataFrameTransformer()),\n",
    "                                    ('drop_text', DropTextTransformer(text_col=2))\n",
    "                                                        ]))\n",
    "                                                            ]))\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the Google word2vec model in place of the tf-idf and PCA steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-28 15:04:38,277 : INFO : loading projection weights from ~/Documents/GoogleNews-vectors-negative300.bin.gz\n",
      "2017-05-28 15:07:19,887 : INFO : loaded (3000000, 300) matrix from ~/Documents/GoogleNews-vectors-negative300.bin.gz\n"
     ]
    }
   ],
   "source": [
    "# ONLY RUN ONCE AT THE START OF THE KERNEL\n",
    "w2v = models.KeyedVectors.load_word2vec_format(\"~/Documents/GoogleNews-vectors-negative300.bin.gz\",binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe_w2v = Pipeline([\n",
    "                                ('combined_features', FeatureUnion([\n",
    "\n",
    "                                    ('num_feat', SeparateFeaturesTransformer(num_cols=['rating'])),\n",
    "                                    ('text_feat', Pipeline([\n",
    "\n",
    "                                        ('split_text', SeparateFeaturesTransformer(text_cols=['reviews'])),\n",
    "                                        ('clean', CleanTextTransformer('reviews')),\n",
    "                                        ('sentiment', SentimentTransformer(text_col='clean_reviews')),\n",
    "                                        ('vectorize', Word2VecTransformer(text_col='clean_reviews', w2v=w2v))\n",
    "                                                            ]))\n",
    "                                                                    ]))\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to take a closer look at the text processing steps, a smaller pipeline will be created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the text processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_processing = Pipeline([\n",
    "                ('clean', CleanTextTransformer('reviews')),\n",
    "                ('to_series', DataframeToSeriesTransformer(col='clean_reviews')),\n",
    "                ('tfidf', TfidfVectorizer(stop_words=\"english\",\n",
    "                                          token_pattern=\"\\\\b[a-zA-Z][a-zA-Z]+\\\\b\",\n",
    "                                          min_df=10)),\n",
    "                ('densify', DensifyTransformer())\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tp = text_processing.fit_transform(df_test)\n",
    "tp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run PCA with 2 latent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tpm = pca.fit_transform(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(tpm[:,0], tpm[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try various numbers of latent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pca_range = [i for i in range(40)]\n",
    "pca_variances_cum = []\n",
    "\n",
    "for num in pca_range:\n",
    "    pca_obj = PCA(n_components=num)\n",
    "    pca_fit = pca_obj.fit_transform(tp)\n",
    "    \n",
    "    pca_variances_cum.append(sum(pca_obj.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1, figsize=(8,6))\n",
    "\n",
    "left  = 0.125  # the left side of the subplots of the figure\n",
    "right = 0.9    # the right side of the subplots of the figure\n",
    "bottom = 0.1   # the bottom of the subplots of the figure\n",
    "top = 0.9      # the top of the subplots of the figure\n",
    "wspace = 0.2   # the amount of width reserved for blank space between subplots\n",
    "hspace = 0.5   # the amount of height reserved for white space between subplots\n",
    "\n",
    "fig.subplots_adjust(left=left, bottom=bottom, right=right, top=top, wspace=wspace, hspace=hspace)\n",
    "\n",
    "# Plot of explained variance\n",
    "ax1 = ax[0]\n",
    "ax1.bar([x for x in range(len(pca_obj.explained_variance_ratio_))],pca_obj.explained_variance_ratio_)\n",
    "\n",
    "ax1.set_title(\"Explained Variance\", size=18)\n",
    "ax1.set_ylabel(\"Percent Explained\", size=16)\n",
    "ax1.set_xlabel(\"Principal Component\", size=16)\n",
    "\n",
    "# Plot of cumulative explained variance\n",
    "ax2 = ax[1]\n",
    "ax2.plot([x for x in range(len(pca_variances_cum))], pca_variances_cum)\n",
    "\n",
    "ax2.set_title(\"Cumulative Explained Variance\", size=18)\n",
    "ax2.set_ylabel(\"Percent Explained\", size=16)\n",
    "ax2.set_xlabel(\"Principal Component\", size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauge how the PCA pipeline is by testing on DB Scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=0.5,\n",
    "            min_samples=10,\n",
    "            metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_arr, pred_pca = get_metrics(pipe_pca, db, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauge how the NMF pipeline performs on DB Scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nmf_arr, pred_nmf = get_metrics(pipe_nmf, db, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauge how the W2V pipeline performs on DB Scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w2v_arr, pred_w2v = get_metrics(pipe_w2v, db, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try other clustering algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"agg\": AgglomerativeClustering(n_clusters=5,\n",
    "                                   affinity=\"cosine\",\n",
    "                                   linkage=\"complete\"),\n",
    "    \"birch\": Birch(threshold=0.5,\n",
    "                   n_clusters=5,\n",
    "                   branching_factor=50),\n",
    "    \"db\": DBSCAN(eps=0.5,\n",
    "                 min_samples=10,\n",
    "                 metric=\"euclidean\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on PCA pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_metrics_dict_PCA = model_metrics(model_dict, pipe_pca, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on NMF pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_metrics_dict_NMF = model_metrics(model_dict, pipe_nmf, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on W2V pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agg\n",
      "SFT:  (4155, 1)\n",
      "SFT:  (4155, 1)\n",
      "CTT:  (4155, 1)\n",
      "ST:  (4155, 3)\n",
      "W2V:  (4155, 300)\n",
      "Number of Clusters:  5\n",
      "Silhouette Coefficient: 0.622\n",
      "birch\n",
      "SFT:  (4155, 1)\n",
      "SFT:  (4155, 1)\n",
      "CTT:  (4155, 1)\n",
      "ST:  (4155, 3)\n",
      "W2V:  (4155, 300)\n",
      "Number of Clusters:  5\n",
      "Silhouette Coefficient: 0.769\n",
      "db\n",
      "SFT:  (4155, 1)\n",
      "SFT:  (4155, 1)\n",
      "CTT:  (4155, 1)\n",
      "ST:  (4155, 3)\n",
      "W2V:  (4155, 300)\n",
      "Number of Clusters:  6\n",
      "Silhouette Coefficient: 0.760\n"
     ]
    }
   ],
   "source": [
    "model_metrics_dict_W2V = model_metrics(model_dict, pipe_w2v, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/df_best']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_best = pd.DataFrame(model_metrics_dict_W2V['birch'][0], columns=['rating'] + [i for i in range(1,301)])\n",
    "df_best['name'] = df_test['name'].tolist()\n",
    "df_best['reviews'] = df_test['reviews'].tolist()\n",
    "\n",
    "joblib.dump(df_best, '../data/df_best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale up by using df_init instead of df_test. This can be done on AWS"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
